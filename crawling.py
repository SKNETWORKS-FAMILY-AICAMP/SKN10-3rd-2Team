# blog_crawler.py

import requests
from bs4 import BeautifulSoup, XMLParsedAsHTMLWarning
from urllib.parse import urlparse, parse_qs, urljoin
import pandas as pd
from fake_useragent import UserAgent
from tqdm import tqdm
import time, random, warnings, re

# ê²½ê³  ì œê±°
warnings.filterwarnings("ignore", category=XMLParsedAsHTMLWarning)

# ì„¸ì…˜ ì„¤ì •
ua = UserAgent()
session = requests.Session()
session.headers.update({
    "User-Agent": ua.chrome,
    "Referer": "https://www.google.com"
})

def convert_to_mobile_naver(url):
    try:
        parsed = urlparse(url)
        if "blog.naver.com" in parsed.netloc and "PostView.naver" in parsed.path:
            q = parse_qs(parsed.query)
            blog_id = q.get("blogId", [""])[0]
            log_no = q.get("logNo", [""])[0]
            if blog_id and log_no:
                return f"https://m.blog.naver.com/{blog_id}/{log_no}"
    except:
        pass
    return url

def is_single_post_url(url):
    return (
        "logNo=" in url or
        re.search(r'tistory\.com/\d+$', url) is not None
    )

def fetch_post_content(url):
    try:
        url = convert_to_mobile_naver(url)
        res = session.get(url, timeout=5)
        soup = BeautifulSoup(res.text, 'html.parser')
        title = soup.title.text.strip() if soup.title else "ì œëª© ì—†ìŒ"

        content = (
            soup.find("div", class_="se-main-container") or
            soup.find("div", id="postViewArea") or
            soup.find("div", class_="post_ct") or
            soup.find("div", class_="view")
        )
        content_text = content.get_text(separator='\n').strip() if content else "ë³¸ë¬¸ ì—†ìŒ"
        return title, content_text
    except Exception as e:
        print(f"[ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨] {url} / {e}")
        return None, None

def extract_post_links(blog_url):
    post_links = set()
    try:
        res = session.get(blog_url, timeout=5)
        soup = BeautifulSoup(res.text, "html.parser")
        for a in soup.find_all('a', href=True):
            href = a['href']
            if not href or href.startswith("#"):
                continue
            full_url = urljoin(blog_url, href)
            if any(x in full_url for x in ['entry', 'post', 'logNo', 'tistory.com']):
                post_links.add(convert_to_mobile_naver(full_url))
    except Exception as e:
        print(f"[ë§í¬ ì¶”ì¶œ ì‹¤íŒ¨] {blog_url} / {e}")
    return list(post_links)

def crawl_all_blogs(blog_list):
    rows = []
    for blog_url in tqdm(blog_list):
        print(f"\nğŸ“Œ í¬ë¡¤ë§ ì‹œì‘: {blog_url}")
        post_links = []

        if is_single_post_url(blog_url):
            post_links = [blog_url]
        else:
            post_links = extract_post_links(blog_url)

        print(f"  â†³ ê²Œì‹œê¸€ ìˆ˜ ì¶”ì¶œë¨: {len(post_links)}")

        for link in post_links:
            title, content = fetch_post_content(link)
            if title and content:
                rows.append({
                    "ë¸”ë¡œê·¸ì£¼ì†Œ": blog_url,
                    "ê²Œì‹œê¸€ì œëª©": title,
                    "ê²Œì‹œê¸€ë‚´ìš©": content
                })
            time.sleep(random.uniform(1.0, 2.2))
    return pd.DataFrame(rows)



if __name__ == "__main__":
    import argparse

    # ê¸°ë³¸ CSV íŒŒì¼ ê²½ë¡œ ì„¤ì •
    default_input = r"D:\ë¶€íŠ¸ìº í”„\MiniProject\SKN10-3rd-2Team\data\ë¸”ë¡œê·¸ ì •ë¦¬ ì·¨í•©ë³¸.csv"
    
    parser = argparse.ArgumentParser(description="ë¸”ë¡œê·¸ ì£¼ì†Œ CSVì—ì„œ íšŒê³  í¬ë¡¤ë§")
    parser.add_argument("--input", type=str, default=default_input, help="ë¸”ë¡œê·¸ ì£¼ì†Œ CSV ê²½ë¡œ (ì»¬ëŸ¼: ë¸”ë¡œê·¸ì£¼ì†Œ)")
    parser.add_argument("--output", type=str, default="í¬ë¡¤ë§_ê²°ê³¼.csv", help="ê²°ê³¼ ì €ì¥ íŒŒì¼ëª…")
    args = parser.parse_args()

    df_links = pd.read_csv(args.input)
    blog_list = df_links['ë¸”ë¡œê·¸ì£¼ì†Œ'].dropna().unique().tolist()

    df_result = crawl_all_blogs(blog_list)
    post_counts = df_result.groupby("ë¸”ë¡œê·¸ì£¼ì†Œ").size().reset_index(name="ê²Œì‹œê¸€ìˆ˜")
    df_final = pd.merge(df_result, post_counts, on="ë¸”ë¡œê·¸ì£¼ì†Œ")

    df_final.to_csv(args.output, index=False)
    print(f"\nâœ… í¬ë¡¤ë§ ì™„ë£Œ: {args.output}")
