{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf032207",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting requests\n",
      "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting beautifulsoup4\n",
      "  Using cached beautifulsoup4-4.13.3-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting pandas\n",
      "  Using cached pandas-2.2.3-cp312-cp312-win_amd64.whl.metadata (19 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests)\n",
      "  Using cached charset_normalizer-3.4.1-cp312-cp312-win_amd64.whl.metadata (36 kB)\n",
      "Collecting idna<4,>=2.5 (from requests)\n",
      "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests)\n",
      "  Using cached urllib3-2.3.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests)\n",
      "  Using cached certifi-2025.1.31-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting soupsieve>1.2 (from beautifulsoup4)\n",
      "  Using cached soupsieve-2.6-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting typing-extensions>=4.0.0 (from beautifulsoup4)\n",
      "  Using cached typing_extensions-4.13.1-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting numpy>=1.26.0 (from pandas)\n",
      "  Using cached numpy-2.2.4-cp312-cp312-win_amd64.whl.metadata (60 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in .\\venv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Using cached pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Using cached tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in .\\venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Using cached beautifulsoup4-4.13.3-py3-none-any.whl (186 kB)\n",
      "Using cached pandas-2.2.3-cp312-cp312-win_amd64.whl (11.5 MB)\n",
      "Using cached certifi-2025.1.31-py3-none-any.whl (166 kB)\n",
      "Using cached charset_normalizer-3.4.1-cp312-cp312-win_amd64.whl (102 kB)\n",
      "Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
      "Using cached numpy-2.2.4-cp312-cp312-win_amd64.whl (12.6 MB)\n",
      "Using cached pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Using cached soupsieve-2.6-py3-none-any.whl (36 kB)\n",
      "Using cached typing_extensions-4.13.1-py3-none-any.whl (45 kB)\n",
      "Using cached tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Using cached urllib3-2.3.0-py3-none-any.whl (128 kB)\n",
      "Installing collected packages: pytz, urllib3, tzdata, typing-extensions, soupsieve, numpy, idna, charset-normalizer, certifi, requests, pandas, beautifulsoup4\n",
      "Successfully installed beautifulsoup4-4.13.3 certifi-2025.1.31 charset-normalizer-3.4.1 idna-3.10 numpy-2.2.4 pandas-2.2.3 pytz-2025.2 requests-2.32.3 soupsieve-2.6 typing-extensions-4.13.1 tzdata-2025.2 urllib3-2.3.0\n"
     ]
    }
   ],
   "source": [
    "pip install requests beautifulsoup4 pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f44b4afa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ë¹„ì „ê³µì ì½”ë”© ë„ì „ê¸°] Page 1: Found 31 blog items\n",
      "[ë¹„ì „ê³µì ì½”ë”© ë„ì „ê¸°] Page 2: Found 31 blog items\n",
      "[ë¹„ì „ê³µì ì½”ë”© ë„ì „ê¸°] Page 3: Found 31 blog items\n",
      "[ë¹„ì „ê³µì ë¶€íŠ¸ìº í”„ í›„ê¸°] Page 1: Found 38 blog items\n",
      "[ë¹„ì „ê³µì ë¶€íŠ¸ìº í”„ í›„ê¸°] Page 2: Found 38 blog items\n",
      "[ë¹„ì „ê³µì ë¶€íŠ¸ìº í”„ í›„ê¸°] Page 3: Found 38 blog items\n",
      "[ë¶€íŠ¸ìº í”„ 1ì£¼ì°¨ ë¸Œì´ë¡œê·¸] Page 1: Found 23 blog items\n",
      "[ë¶€íŠ¸ìº í”„ 1ì£¼ì°¨ ë¸Œì´ë¡œê·¸] Page 2: Found 23 blog items\n",
      "[ë¶€íŠ¸ìº í”„ 1ì£¼ì°¨ ë¸Œì´ë¡œê·¸] Page 3: Found 23 blog items\n",
      "[ë‚´ì¼ë°°ì›€ìº í”„ í›„ê¸°] Page 1: Found 13 blog items\n",
      "[ë‚´ì¼ë°°ì›€ìº í”„ í›„ê¸°] Page 2: Found 13 blog items\n",
      "[ë‚´ì¼ë°°ì›€ìº í”„ í›„ê¸°] Page 3: Found 13 blog items\n",
      "[í•­í•´99 1ì£¼ì°¨ íšŒê³ ] Page 1: Found 28 blog items\n",
      "[í•­í•´99 1ì£¼ì°¨ íšŒê³ ] Page 2: Found 28 blog items\n",
      "[í•­í•´99 1ì£¼ì°¨ íšŒê³ ] Page 3: Found 28 blog items\n",
      "[ê°œë°œì ì·¨ì—… êµ­ë¹„ì§€ì› í›„ê¸°] Page 1: Found 34 blog items\n",
      "[ê°œë°œì ì·¨ì—… êµ­ë¹„ì§€ì› í›„ê¸°] Page 2: Found 34 blog items\n",
      "[ê°œë°œì ì·¨ì—… êµ­ë¹„ì§€ì› í›„ê¸°] Page 3: Found 34 blog items\n",
      "[êµ­ë¹„êµìœ¡ ì§„ì§œ í›„ê¸°] Page 1: Found 32 blog items\n",
      "[êµ­ë¹„êµìœ¡ ì§„ì§œ í›„ê¸°] Page 2: Found 32 blog items\n",
      "[êµ­ë¹„êµìœ¡ ì§„ì§œ í›„ê¸°] Page 3: Found 32 blog items\n",
      "[ë¹„ì „ê³µì íŒŒì´ì¬ ê³µë¶€] Page 1: Found 34 blog items\n",
      "[ë¹„ì „ê³µì íŒŒì´ì¬ ê³µë¶€] Page 2: Found 34 blog items\n",
      "[ë¹„ì „ê³µì íŒŒì´ì¬ ê³µë¶€] Page 3: Found 34 blog items\n",
      "[ê°œë°œì ì „í–¥ ë¸Œì´ë¡œê·¸] Page 1: Found 27 blog items\n",
      "[ê°œë°œì ì „í–¥ ë¸Œì´ë¡œê·¸] Page 2: Found 27 blog items\n",
      "[ê°œë°œì ì „í–¥ ë¸Œì´ë¡œê·¸] Page 3: Found 27 blog items\n",
      "[ë¹„ì „ê³µì ì½”ë”© ê°€ëŠ¥í• ê¹Œ] Page 1: Found 33 blog items\n",
      "[ë¹„ì „ê³µì ì½”ë”© ê°€ëŠ¥í• ê¹Œ] Page 2: Found 33 blog items\n",
      "[ë¹„ì „ê³µì ì½”ë”© ê°€ëŠ¥í• ê¹Œ] Page 3: Found 33 blog items\n",
      "[HTMLë¶€í„° ë°°ìš°ê¸° ë¹„ì „ê³µì] Page 1: Found 35 blog items\n",
      "[HTMLë¶€í„° ë°°ìš°ê¸° ë¹„ì „ê³µì] Page 2: Found 35 blog items\n",
      "[HTMLë¶€í„° ë°°ìš°ê¸° ë¹„ì „ê³µì] Page 3: Found 35 blog items\n",
      "[ì»´ê³µ ì•„ë‹Œë° ê°œë°œì ë˜ê¸°] Page 1: Found 26 blog items\n",
      "[ì»´ê³µ ì•„ë‹Œë° ê°œë°œì ë˜ê¸°] Page 2: Found 26 blog items\n",
      "[ì»´ê³µ ì•„ë‹Œë° ê°œë°œì ë˜ê¸°] Page 3: Found 26 blog items\n",
      "âœ… ë³¸ë¬¸ í¬í•¨ í¬ë¡¤ë§ ì™„ë£Œ! 'naver_blog_full.csv' ì €ì¥ë¨.\n",
      "[ë¹„ì „ê³µì ì½”ë”© ë„ì „ê¸°] Page 1: Found 31 blog items\n",
      "[ë¹„ì „ê³µì ì½”ë”© ë„ì „ê¸°] Page 2: Found 31 blog items\n",
      "[ë¹„ì „ê³µì ì½”ë”© ë„ì „ê¸°] Page 3: Found 31 blog items\n",
      "[ë¹„ì „ê³µì ë¶€íŠ¸ìº í”„ í›„ê¸°] Page 1: Found 38 blog items\n",
      "[ë¹„ì „ê³µì ë¶€íŠ¸ìº í”„ í›„ê¸°] Page 2: Found 38 blog items\n",
      "[ë¹„ì „ê³µì ë¶€íŠ¸ìº í”„ í›„ê¸°] Page 3: Found 38 blog items\n",
      "[ë¶€íŠ¸ìº í”„ 1ì£¼ì°¨ ë¸Œì´ë¡œê·¸] Page 1: Found 25 blog items\n",
      "[ë¶€íŠ¸ìº í”„ 1ì£¼ì°¨ ë¸Œì´ë¡œê·¸] Page 2: Found 25 blog items\n",
      "[ë¶€íŠ¸ìº í”„ 1ì£¼ì°¨ ë¸Œì´ë¡œê·¸] Page 3: Found 25 blog items\n",
      "[ë‚´ì¼ë°°ì›€ìº í”„ í›„ê¸°] Page 1: Found 13 blog items\n",
      "[ë‚´ì¼ë°°ì›€ìº í”„ í›„ê¸°] Page 2: Found 13 blog items\n",
      "[ë‚´ì¼ë°°ì›€ìº í”„ í›„ê¸°] Page 3: Found 13 blog items\n",
      "[í•­í•´99 1ì£¼ì°¨ íšŒê³ ] Page 1: Found 28 blog items\n",
      "[í•­í•´99 1ì£¼ì°¨ íšŒê³ ] Page 2: Found 28 blog items\n",
      "[í•­í•´99 1ì£¼ì°¨ íšŒê³ ] Page 3: Found 28 blog items\n",
      "[ê°œë°œì ì·¨ì—… êµ­ë¹„ì§€ì› í›„ê¸°] Page 1: Found 34 blog items\n",
      "[ê°œë°œì ì·¨ì—… êµ­ë¹„ì§€ì› í›„ê¸°] Page 2: Found 34 blog items\n",
      "[ê°œë°œì ì·¨ì—… êµ­ë¹„ì§€ì› í›„ê¸°] Page 3: Found 34 blog items\n",
      "[êµ­ë¹„êµìœ¡ ì§„ì§œ í›„ê¸°] Page 1: Found 32 blog items\n",
      "[êµ­ë¹„êµìœ¡ ì§„ì§œ í›„ê¸°] Page 2: Found 32 blog items\n",
      "[êµ­ë¹„êµìœ¡ ì§„ì§œ í›„ê¸°] Page 3: Found 32 blog items\n",
      "[ë¹„ì „ê³µì íŒŒì´ì¬ ê³µë¶€] Page 1: Found 34 blog items\n",
      "[ë¹„ì „ê³µì íŒŒì´ì¬ ê³µë¶€] Page 2: Found 34 blog items\n",
      "[ë¹„ì „ê³µì íŒŒì´ì¬ ê³µë¶€] Page 3: Found 34 blog items\n",
      "[ê°œë°œì ì „í–¥ ë¸Œì´ë¡œê·¸] Page 1: Found 27 blog items\n",
      "[ê°œë°œì ì „í–¥ ë¸Œì´ë¡œê·¸] Page 2: Found 27 blog items\n",
      "[ê°œë°œì ì „í–¥ ë¸Œì´ë¡œê·¸] Page 3: Found 27 blog items\n",
      "[ë¹„ì „ê³µì ì½”ë”© ê°€ëŠ¥í• ê¹Œ] Page 1: Found 34 blog items\n",
      "[ë¹„ì „ê³µì ì½”ë”© ê°€ëŠ¥í• ê¹Œ] Page 2: Found 34 blog items\n",
      "[ë¹„ì „ê³µì ì½”ë”© ê°€ëŠ¥í• ê¹Œ] Page 3: Found 34 blog items\n",
      "[HTMLë¶€í„° ë°°ìš°ê¸° ë¹„ì „ê³µì] Page 1: Found 35 blog items\n",
      "[HTMLë¶€í„° ë°°ìš°ê¸° ë¹„ì „ê³µì] Page 2: Found 35 blog items\n",
      "[HTMLë¶€í„° ë°°ìš°ê¸° ë¹„ì „ê³µì] Page 3: Found 35 blog items\n",
      "[ì»´ê³µ ì•„ë‹Œë° ê°œë°œì ë˜ê¸°] Page 1: Found 26 blog items\n",
      "[ì»´ê³µ ì•„ë‹Œë° ê°œë°œì ë˜ê¸°] Page 2: Found 26 blog items\n",
      "[ì»´ê³µ ì•„ë‹Œë° ê°œë°œì ë˜ê¸°] Page 3: Found 26 blog items\n",
      "âœ… í¬ë¡¤ë§ ì™„ë£Œ! naver_blog_results.csv ì €ì¥ë¨\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "def get_blog_full_text(url):\n",
    "    try:\n",
    "        headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "        res = requests.get(url, headers=headers, timeout=5)\n",
    "        soup = BeautifulSoup(res.text, 'html.parser')\n",
    "\n",
    "        # ë„¤ì´ë²„ ë¸”ë¡œê·¸ì˜ iframe êµ¬ì¡° ì²˜ë¦¬\n",
    "        if \"blog.naver.com\" in url and \"PostView.naver\" not in res.url:\n",
    "            iframe = soup.find(\"iframe\")\n",
    "            if iframe:\n",
    "                iframe_url = \"https://blog.naver.com\" + iframe[\"src\"]\n",
    "                res = requests.get(iframe_url, headers=headers, timeout=5)\n",
    "                soup = BeautifulSoup(res.text, 'html.parser')\n",
    "\n",
    "        # âœ… ë³¸ë¬¸ ì¶”ì¶œ ì‹œë„ (ìµœì‹  êµ¬ì¡° ëŒ€ì‘)\n",
    "        main_content = soup.select_one(\"div.se-main-container\")  # ì—ë””í„° 3.0\n",
    "        if not main_content:\n",
    "            main_content = soup.select_one(\"div#postViewArea\")  # êµ¬í˜• ì—ë””í„° ëŒ€ì‘\n",
    "\n",
    "        if main_content:\n",
    "            text = main_content.get_text(separator=\"\\n\").strip()\n",
    "            return text\n",
    "        else:\n",
    "            return \"\"\n",
    "    except Exception as e:\n",
    "        return \"\"\n",
    "\n",
    "def crawl_naver_blog(keyword, max_pages=3):\n",
    "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "    results = []\n",
    "\n",
    "    for page in range(1, max_pages + 1):\n",
    "        start = (page - 1) * 10 + 1\n",
    "        url = f\"https://search.naver.com/search.naver?query={keyword}&where=post&sm=tab_pge&start={start}\"\n",
    "        res = requests.get(url, headers=headers)\n",
    "        soup = BeautifulSoup(res.text, 'html.parser')\n",
    "\n",
    "        items = soup.select(\"li.bx\")\n",
    "        print(f\"[{keyword}] Page {page}: Found {len(items)} blog items\")\n",
    "\n",
    "        for item in items:\n",
    "            title_tag = item.select_one(\"a.api_txt_lines\")\n",
    "            if title_tag:\n",
    "                title = title_tag.text.strip()\n",
    "                link = title_tag['href']\n",
    "                body_text = get_blog_full_text(link)\n",
    "                results.append({'keyword': keyword, 'title': title, 'link': link, 'body': body_text})\n",
    "\n",
    "        time.sleep(1)\n",
    "\n",
    "    return results\n",
    "\n",
    "# ì‹¤í–‰\n",
    "keywords = [\n",
    "    \"ë¹„ì „ê³µì ì½”ë”© ë„ì „ê¸°\", \"ë¹„ì „ê³µì ë¶€íŠ¸ìº í”„ í›„ê¸°\", \"ë¶€íŠ¸ìº í”„ 1ì£¼ì°¨ ë¸Œì´ë¡œê·¸\",\n",
    "    \"ë‚´ì¼ë°°ì›€ìº í”„ í›„ê¸°\", \"í•­í•´99 1ì£¼ì°¨ íšŒê³ \", \"ê°œë°œì ì·¨ì—… êµ­ë¹„ì§€ì› í›„ê¸°\",\n",
    "    \"êµ­ë¹„êµìœ¡ ì§„ì§œ í›„ê¸°\", \"ë¹„ì „ê³µì íŒŒì´ì¬ ê³µë¶€\", \"ê°œë°œì ì „í–¥ ë¸Œì´ë¡œê·¸\",\n",
    "    \"ë¹„ì „ê³µì ì½”ë”© ê°€ëŠ¥í• ê¹Œ\", \"HTMLë¶€í„° ë°°ìš°ê¸° ë¹„ì „ê³µì\", \"ì»´ê³µ ì•„ë‹Œë° ê°œë°œì ë˜ê¸°\"\n",
    "]\n",
    "all_results = []\n",
    "\n",
    "for kw in keywords:\n",
    "    all_results.extend(crawl_naver_blog(kw, max_pages=3))\n",
    "\n",
    "df = pd.DataFrame(all_results)\n",
    "df.to_csv(\"naver_blog_full.csv\", index=False, encoding='utf-8-sig')\n",
    "print(\"âœ… ë³¸ë¬¸ í¬í•¨ í¬ë¡¤ë§ ì™„ë£Œ! 'naver_blog_full.csv' ì €ì¥ë¨.\")\n",
    "\n",
    "\n",
    "# ğŸ” ëŒ€ìƒ í‚¤ì›Œë“œ\n",
    "keywords = [\n",
    "    \"ë¹„ì „ê³µì ì½”ë”© ë„ì „ê¸°\", \"ë¹„ì „ê³µì ë¶€íŠ¸ìº í”„ í›„ê¸°\", \"ë¶€íŠ¸ìº í”„ 1ì£¼ì°¨ ë¸Œì´ë¡œê·¸\",\n",
    "    \"ë‚´ì¼ë°°ì›€ìº í”„ í›„ê¸°\", \"í•­í•´99 1ì£¼ì°¨ íšŒê³ \", \"ê°œë°œì ì·¨ì—… êµ­ë¹„ì§€ì› í›„ê¸°\",\n",
    "    \"êµ­ë¹„êµìœ¡ ì§„ì§œ í›„ê¸°\", \"ë¹„ì „ê³µì íŒŒì´ì¬ ê³µë¶€\", \"ê°œë°œì ì „í–¥ ë¸Œì´ë¡œê·¸\",\n",
    "    \"ë¹„ì „ê³µì ì½”ë”© ê°€ëŠ¥í• ê¹Œ\", \"HTMLë¶€í„° ë°°ìš°ê¸° ë¹„ì „ê³µì\", \"ì»´ê³µ ì•„ë‹Œë° ê°œë°œì ë˜ê¸°\"\n",
    "]\n",
    "all_results = []\n",
    "\n",
    "for kw in keywords:\n",
    "    all_results.extend(crawl_naver_blog(kw, max_pages=3))\n",
    "\n",
    "# ğŸ’¾ ì €ì¥\n",
    "df = pd.DataFrame(all_results)\n",
    "df.to_csv(\"naver_blog_results.csv\", index=False, encoding='utf-8-sig')\n",
    "print(\"âœ… í¬ë¡¤ë§ ì™„ë£Œ! naver_blog_results.csv ì €ì¥ë¨\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
