{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf032207",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting requests\n",
      "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting beautifulsoup4\n",
      "  Using cached beautifulsoup4-4.13.3-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting pandas\n",
      "  Using cached pandas-2.2.3-cp312-cp312-win_amd64.whl.metadata (19 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests)\n",
      "  Using cached charset_normalizer-3.4.1-cp312-cp312-win_amd64.whl.metadata (36 kB)\n",
      "Collecting idna<4,>=2.5 (from requests)\n",
      "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests)\n",
      "  Using cached urllib3-2.3.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests)\n",
      "  Using cached certifi-2025.1.31-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting soupsieve>1.2 (from beautifulsoup4)\n",
      "  Using cached soupsieve-2.6-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting typing-extensions>=4.0.0 (from beautifulsoup4)\n",
      "  Using cached typing_extensions-4.13.1-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting numpy>=1.26.0 (from pandas)\n",
      "  Using cached numpy-2.2.4-cp312-cp312-win_amd64.whl.metadata (60 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in .\\venv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Using cached pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Using cached tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in .\\venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Using cached beautifulsoup4-4.13.3-py3-none-any.whl (186 kB)\n",
      "Using cached pandas-2.2.3-cp312-cp312-win_amd64.whl (11.5 MB)\n",
      "Using cached certifi-2025.1.31-py3-none-any.whl (166 kB)\n",
      "Using cached charset_normalizer-3.4.1-cp312-cp312-win_amd64.whl (102 kB)\n",
      "Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
      "Using cached numpy-2.2.4-cp312-cp312-win_amd64.whl (12.6 MB)\n",
      "Using cached pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Using cached soupsieve-2.6-py3-none-any.whl (36 kB)\n",
      "Using cached typing_extensions-4.13.1-py3-none-any.whl (45 kB)\n",
      "Using cached tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Using cached urllib3-2.3.0-py3-none-any.whl (128 kB)\n",
      "Installing collected packages: pytz, urllib3, tzdata, typing-extensions, soupsieve, numpy, idna, charset-normalizer, certifi, requests, pandas, beautifulsoup4\n",
      "Successfully installed beautifulsoup4-4.13.3 certifi-2025.1.31 charset-normalizer-3.4.1 idna-3.10 numpy-2.2.4 pandas-2.2.3 pytz-2025.2 requests-2.32.3 soupsieve-2.6 typing-extensions-4.13.1 tzdata-2025.2 urllib3-2.3.0\n"
     ]
    }
   ],
   "source": [
    "pip install requests beautifulsoup4 pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f44b4afa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[비전공자 코딩 도전기] Page 1: Found 31 blog items\n",
      "[비전공자 코딩 도전기] Page 2: Found 31 blog items\n",
      "[비전공자 코딩 도전기] Page 3: Found 31 blog items\n",
      "[비전공자 부트캠프 후기] Page 1: Found 38 blog items\n",
      "[비전공자 부트캠프 후기] Page 2: Found 38 blog items\n",
      "[비전공자 부트캠프 후기] Page 3: Found 38 blog items\n",
      "[부트캠프 1주차 브이로그] Page 1: Found 23 blog items\n",
      "[부트캠프 1주차 브이로그] Page 2: Found 23 blog items\n",
      "[부트캠프 1주차 브이로그] Page 3: Found 23 blog items\n",
      "[내일배움캠프 후기] Page 1: Found 13 blog items\n",
      "[내일배움캠프 후기] Page 2: Found 13 blog items\n",
      "[내일배움캠프 후기] Page 3: Found 13 blog items\n",
      "[항해99 1주차 회고] Page 1: Found 28 blog items\n",
      "[항해99 1주차 회고] Page 2: Found 28 blog items\n",
      "[항해99 1주차 회고] Page 3: Found 28 blog items\n",
      "[개발자 취업 국비지원 후기] Page 1: Found 34 blog items\n",
      "[개발자 취업 국비지원 후기] Page 2: Found 34 blog items\n",
      "[개발자 취업 국비지원 후기] Page 3: Found 34 blog items\n",
      "[국비교육 진짜 후기] Page 1: Found 32 blog items\n",
      "[국비교육 진짜 후기] Page 2: Found 32 blog items\n",
      "[국비교육 진짜 후기] Page 3: Found 32 blog items\n",
      "[비전공자 파이썬 공부] Page 1: Found 34 blog items\n",
      "[비전공자 파이썬 공부] Page 2: Found 34 blog items\n",
      "[비전공자 파이썬 공부] Page 3: Found 34 blog items\n",
      "[개발자 전향 브이로그] Page 1: Found 27 blog items\n",
      "[개발자 전향 브이로그] Page 2: Found 27 blog items\n",
      "[개발자 전향 브이로그] Page 3: Found 27 blog items\n",
      "[비전공자 코딩 가능할까] Page 1: Found 33 blog items\n",
      "[비전공자 코딩 가능할까] Page 2: Found 33 blog items\n",
      "[비전공자 코딩 가능할까] Page 3: Found 33 blog items\n",
      "[HTML부터 배우기 비전공자] Page 1: Found 35 blog items\n",
      "[HTML부터 배우기 비전공자] Page 2: Found 35 blog items\n",
      "[HTML부터 배우기 비전공자] Page 3: Found 35 blog items\n",
      "[컴공 아닌데 개발자 되기] Page 1: Found 26 blog items\n",
      "[컴공 아닌데 개발자 되기] Page 2: Found 26 blog items\n",
      "[컴공 아닌데 개발자 되기] Page 3: Found 26 blog items\n",
      "✅ 본문 포함 크롤링 완료! 'naver_blog_full.csv' 저장됨.\n",
      "[비전공자 코딩 도전기] Page 1: Found 31 blog items\n",
      "[비전공자 코딩 도전기] Page 2: Found 31 blog items\n",
      "[비전공자 코딩 도전기] Page 3: Found 31 blog items\n",
      "[비전공자 부트캠프 후기] Page 1: Found 38 blog items\n",
      "[비전공자 부트캠프 후기] Page 2: Found 38 blog items\n",
      "[비전공자 부트캠프 후기] Page 3: Found 38 blog items\n",
      "[부트캠프 1주차 브이로그] Page 1: Found 25 blog items\n",
      "[부트캠프 1주차 브이로그] Page 2: Found 25 blog items\n",
      "[부트캠프 1주차 브이로그] Page 3: Found 25 blog items\n",
      "[내일배움캠프 후기] Page 1: Found 13 blog items\n",
      "[내일배움캠프 후기] Page 2: Found 13 blog items\n",
      "[내일배움캠프 후기] Page 3: Found 13 blog items\n",
      "[항해99 1주차 회고] Page 1: Found 28 blog items\n",
      "[항해99 1주차 회고] Page 2: Found 28 blog items\n",
      "[항해99 1주차 회고] Page 3: Found 28 blog items\n",
      "[개발자 취업 국비지원 후기] Page 1: Found 34 blog items\n",
      "[개발자 취업 국비지원 후기] Page 2: Found 34 blog items\n",
      "[개발자 취업 국비지원 후기] Page 3: Found 34 blog items\n",
      "[국비교육 진짜 후기] Page 1: Found 32 blog items\n",
      "[국비교육 진짜 후기] Page 2: Found 32 blog items\n",
      "[국비교육 진짜 후기] Page 3: Found 32 blog items\n",
      "[비전공자 파이썬 공부] Page 1: Found 34 blog items\n",
      "[비전공자 파이썬 공부] Page 2: Found 34 blog items\n",
      "[비전공자 파이썬 공부] Page 3: Found 34 blog items\n",
      "[개발자 전향 브이로그] Page 1: Found 27 blog items\n",
      "[개발자 전향 브이로그] Page 2: Found 27 blog items\n",
      "[개발자 전향 브이로그] Page 3: Found 27 blog items\n",
      "[비전공자 코딩 가능할까] Page 1: Found 34 blog items\n",
      "[비전공자 코딩 가능할까] Page 2: Found 34 blog items\n",
      "[비전공자 코딩 가능할까] Page 3: Found 34 blog items\n",
      "[HTML부터 배우기 비전공자] Page 1: Found 35 blog items\n",
      "[HTML부터 배우기 비전공자] Page 2: Found 35 blog items\n",
      "[HTML부터 배우기 비전공자] Page 3: Found 35 blog items\n",
      "[컴공 아닌데 개발자 되기] Page 1: Found 26 blog items\n",
      "[컴공 아닌데 개발자 되기] Page 2: Found 26 blog items\n",
      "[컴공 아닌데 개발자 되기] Page 3: Found 26 blog items\n",
      "✅ 크롤링 완료! naver_blog_results.csv 저장됨\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "def get_blog_full_text(url):\n",
    "    try:\n",
    "        headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "        res = requests.get(url, headers=headers, timeout=5)\n",
    "        soup = BeautifulSoup(res.text, 'html.parser')\n",
    "\n",
    "        # 네이버 블로그의 iframe 구조 처리\n",
    "        if \"blog.naver.com\" in url and \"PostView.naver\" not in res.url:\n",
    "            iframe = soup.find(\"iframe\")\n",
    "            if iframe:\n",
    "                iframe_url = \"https://blog.naver.com\" + iframe[\"src\"]\n",
    "                res = requests.get(iframe_url, headers=headers, timeout=5)\n",
    "                soup = BeautifulSoup(res.text, 'html.parser')\n",
    "\n",
    "        # ✅ 본문 추출 시도 (최신 구조 대응)\n",
    "        main_content = soup.select_one(\"div.se-main-container\")  # 에디터 3.0\n",
    "        if not main_content:\n",
    "            main_content = soup.select_one(\"div#postViewArea\")  # 구형 에디터 대응\n",
    "\n",
    "        if main_content:\n",
    "            text = main_content.get_text(separator=\"\\n\").strip()\n",
    "            return text\n",
    "        else:\n",
    "            return \"\"\n",
    "    except Exception as e:\n",
    "        return \"\"\n",
    "\n",
    "def crawl_naver_blog(keyword, max_pages=3):\n",
    "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "    results = []\n",
    "\n",
    "    for page in range(1, max_pages + 1):\n",
    "        start = (page - 1) * 10 + 1\n",
    "        url = f\"https://search.naver.com/search.naver?query={keyword}&where=post&sm=tab_pge&start={start}\"\n",
    "        res = requests.get(url, headers=headers)\n",
    "        soup = BeautifulSoup(res.text, 'html.parser')\n",
    "\n",
    "        items = soup.select(\"li.bx\")\n",
    "        print(f\"[{keyword}] Page {page}: Found {len(items)} blog items\")\n",
    "\n",
    "        for item in items:\n",
    "            title_tag = item.select_one(\"a.api_txt_lines\")\n",
    "            if title_tag:\n",
    "                title = title_tag.text.strip()\n",
    "                link = title_tag['href']\n",
    "                body_text = get_blog_full_text(link)\n",
    "                results.append({'keyword': keyword, 'title': title, 'link': link, 'body': body_text})\n",
    "\n",
    "        time.sleep(1)\n",
    "\n",
    "    return results\n",
    "\n",
    "# 실행\n",
    "keywords = [\n",
    "    \"비전공자 코딩 도전기\", \"비전공자 부트캠프 후기\", \"부트캠프 1주차 브이로그\",\n",
    "    \"내일배움캠프 후기\", \"항해99 1주차 회고\", \"개발자 취업 국비지원 후기\",\n",
    "    \"국비교육 진짜 후기\", \"비전공자 파이썬 공부\", \"개발자 전향 브이로그\",\n",
    "    \"비전공자 코딩 가능할까\", \"HTML부터 배우기 비전공자\", \"컴공 아닌데 개발자 되기\"\n",
    "]\n",
    "all_results = []\n",
    "\n",
    "for kw in keywords:\n",
    "    all_results.extend(crawl_naver_blog(kw, max_pages=3))\n",
    "\n",
    "df = pd.DataFrame(all_results)\n",
    "df.to_csv(\"naver_blog_full.csv\", index=False, encoding='utf-8-sig')\n",
    "print(\"✅ 본문 포함 크롤링 완료! 'naver_blog_full.csv' 저장됨.\")\n",
    "\n",
    "\n",
    "# 🔍 대상 키워드\n",
    "keywords = [\n",
    "    \"비전공자 코딩 도전기\", \"비전공자 부트캠프 후기\", \"부트캠프 1주차 브이로그\",\n",
    "    \"내일배움캠프 후기\", \"항해99 1주차 회고\", \"개발자 취업 국비지원 후기\",\n",
    "    \"국비교육 진짜 후기\", \"비전공자 파이썬 공부\", \"개발자 전향 브이로그\",\n",
    "    \"비전공자 코딩 가능할까\", \"HTML부터 배우기 비전공자\", \"컴공 아닌데 개발자 되기\"\n",
    "]\n",
    "all_results = []\n",
    "\n",
    "for kw in keywords:\n",
    "    all_results.extend(crawl_naver_blog(kw, max_pages=3))\n",
    "\n",
    "# 💾 저장\n",
    "df = pd.DataFrame(all_results)\n",
    "df.to_csv(\"naver_blog_results.csv\", index=False, encoding='utf-8-sig')\n",
    "print(\"✅ 크롤링 완료! naver_blog_results.csv 저장됨\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
