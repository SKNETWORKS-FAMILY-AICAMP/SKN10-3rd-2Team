import pandas as pd
import requests
from bs4 import BeautifulSoup
from fake_useragent import UserAgent
from urllib.parse import urlparse, parse_qs
import re
import time
import random
import pytesseract
from PIL import Image
import io
import os

# ====== ì„¤ì • ======
INPUT_CSV = 'data/ë¸”ë¡œê·¸1.csv'
OUTPUT_CSV = 'rag_dataset.csv'

# ====== ì´ˆê¸°í™” ======
ua = UserAgent()
session = requests.Session()
session.headers.update({
    "User-Agent": ua.chrome,
    "Referer": "https://www.google.com"
})

# ====== ì´ë¯¸ì§€ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ ======
def extract_text_from_image(image_url):
    try:
        # ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ
        response = session.get(image_url, timeout=5)
        image = Image.open(io.BytesIO(response.content))
        
        # OCR ìˆ˜í–‰
        text = pytesseract.image_to_string(image, lang='kor+eng')
        return text.strip()
    except Exception as e:
        print(f"[ì´ë¯¸ì§€ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨] {image_url} / {e}")
        return ""

# ====== ë„¤ì´ë²„ ì£¼ì†Œ ëª¨ë°”ì¼ë¡œ ë³€í™˜ ======
def convert_to_mobile_naver(url):
    try:
        parsed = urlparse(url)
        if "blog.naver.com" in parsed.netloc and "PostView.naver" in parsed.path:
            q = parse_qs(parsed.query)
            blog_id = q.get("blogId", [""])[0]
            log_no = q.get("logNo", [""])[0]
            if blog_id and log_no:
                return f"https://m.blog.naver.com/{blog_id}/{log_no}"
    except:
        pass
    return url

# ====== ì£¼ì°¨ ì •ë³´ ì¶”ì¶œ ======
def extract_week(title):
    match = re.search(r'(\d)[ ]*ì£¼ì°¨', str(title))
    return int(match.group(1)) if match else None

# ====== ë³¸ë¬¸ í¬ë¡¤ë§ ======
def fetch_post(url):
    url = convert_to_mobile_naver(url)
    try:
        res = session.get(url, timeout=5)
        soup = BeautifulSoup(res.text, 'html.parser')
        title = soup.title.text.strip() if soup.title else "ì œëª© ì—†ìŒ"

        content = (
            soup.find("div", class_="se-main-container") or
            soup.find("div", id="postViewArea") or
            soup.find("div", class_="post_ct") or
            soup.find("div", class_="view")
        )
        
        # í…ìŠ¤íŠ¸ ë‚´ìš© ì¶”ì¶œ
        content_text = content.get_text(separator='\n').strip() if content else "ë³¸ë¬¸ ì—†ìŒ"
        
        # ì´ë¯¸ì§€ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
        image_texts = []
        for img in content.find_all('img'):
            img_url = img.get('src')
            if img_url:
                img_text = extract_text_from_image(img_url)
                if img_text:
                    image_texts.append(img_text)
        
        # í…ìŠ¤íŠ¸ì™€ ì´ë¯¸ì§€ í…ìŠ¤íŠ¸ ê²°í•©
        full_content = content_text
        if image_texts:
            full_content += "\n\n[ì´ë¯¸ì§€ì—ì„œ ì¶”ì¶œëœ í…ìŠ¤íŠ¸]\n" + "\n".join(image_texts)
            
        return title, full_content
    except Exception as e:
        print(f"[í¬ë¡¤ë§ ì‹¤íŒ¨] {url} / {e}")
        return None, None

# ====== ì „ì²´ ì‹¤í–‰ í•¨ìˆ˜ ======
def main():
    df = pd.read_csv(INPUT_CSV)
    url_list = df.iloc[:, 0].dropna().unique().tolist()
    results = []

    for url in url_list:
        print(f"ğŸ“Œ í¬ë¡¤ë§ ì¤‘: {url}")
        title, content = fetch_post(url)
        if not content or content == "ë³¸ë¬¸ ì—†ìŒ":
            continue
        week = extract_week(title)
        paragraphs = [p.strip() for p in content.split('\n') if p.strip()]
        for i, para in enumerate(paragraphs, 1):
            results.append({
                'ë¸”ë¡œê·¸ì£¼ì†Œ': url,
                'ê²Œì‹œê¸€ì œëª©': title,
                'ì£¼ì°¨': week,
                'ë¬¸ë‹¨': para,
                'ë¬¸ë‹¨ìˆœë²ˆ': i
            })
        time.sleep(random.uniform(1.0, 2.0))

    df_out = pd.DataFrame(results)
    df_out.to_csv(OUTPUT_CSV, index=False)
    print(f"\nâœ… í¬ë¡¤ë§ ì™„ë£Œ: {OUTPUT_CSV}")

# ====== ì‹¤í–‰ ======
if __name__ == "__main__":
    main()
